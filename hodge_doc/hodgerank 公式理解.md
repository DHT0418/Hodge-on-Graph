在众包任务中，我们会收集到大量关于不同选项的成对比较数据，就像很多人对不同电影进行两两比较并给出偏好。但这些数据往往存在不平衡（某些电影被比较的次数多，某些少）和不完整（有些电影对没有被比较）的问题。

1. **HodgeRank基础公式解释**
   - 最小二乘法推广的 Borda 计数公式$$min_{x}\left\| y - D_{0}x\right\| _{2}^{2}$$
   我们想找到一个能代表所有选项的全局评分$x$。这里的$y$是我们收集到的成对比较数据，$D_{0}$是一个根据比较数据计算差值的“工具”。这个公式的意思是要找到一个$x$，使得用$D_{0}$计算出来的差值与实际的$y$之间的误差平方和最小，就好像要找到一个最合适的评分标准，让它与大家的比较结果最接近。
   - 图拉普拉斯方程$$D_{0}^{T}D_{0}x = D_{0}^{T}y$$
   这是从前面的最小二乘法问题推导出来的。可以把它想象成一个平衡的过程，$D_{0}^{T}D_{0}$就像是一个调整的“力量”，通过这个方程来找到满足条件的$x$，也就是那个能让整个系统平衡的全局评分。

2. **新的 Hodge 分解公式解释**
   - Hodge 分解定理$$y = b + u + D_{0}x + D_{1}^{T}z + w$$
     - $b$是$y$的对称部分，它表示在比较过程中因为一些固定因素（比如比赛中主场优势）导致的位置偏差。比如在体育比赛中，主场球队可能会有一些额外的优势，这个优势就体现在$b$中。
     - $u$是通用核，它表示所有成对比较完全平局的情况，就好像在投票中大家都觉得两个选项一样好。
     - $D_{0}x$就是我们前面一直在找的那个全局评分部分，它代表了整体的排名趋势。
     - $D_{1}^{T}z$捕获平均三角循环，它反映了数据中的一些循环关系，比如在一些比较复杂的决策过程中，可能会出现循环的偏好情况。
     - $w$是调和排名，它是导致投票混乱的根源，就像在选举中可能会因为一些复杂的关系导致结果很混乱。

   - 三角卷曲（迹）算子$D_{1}$公式$$(D_{1}y)(i,j,k)=\frac{1}{m_{ij}}\sum_{\alpha}y_{ij}^{\alpha}+\frac{1}{m_{jk}}\sum_{\alpha}y_{jk}^{\alpha}+\frac{1}{m_{ki}}\sum_{\alpha}y_{ki}^{\alpha}$$这个公式是用来计算$D_{1}^{T}z$中的$D_{1}$部分的。它把三个相关的成对比较数据综合起来，看看它们之间有没有循环的关系。

3. **统计模型相关公式解释**
   - 广义线性模型假设公式$$\pi_{ij}=Prob\{i\succ j\}=\Phi(x_{i}^{*}-x_{j}^{*})$$
   这里$\pi_{ij}$表示$i$比$j$好的概率。$\Phi$是一个函数，它根据$i$和$j$的某种“潜在分数”$x_{i}^{*}$和$x_{j}^{*}$的差值来确定这个概率。就好像根据两个学生的能力分数差值来判断一个学生比另一个学生成绩好的概率。

4. **Fisher 信息最大化（无监督采样）公式解释**
   - 最大似然问题公式$$max_{x}\frac{(2\pi)^{-m/2}}{det(\sum_{\epsilon})}exp\left(-\frac{1}{2}(y - D_{0}x)^{T}\sum_{\epsilon}^{-1}(y - D_{0}x)\right)$$
   在我们假设成对比较数据中的循环排名是由高斯噪声引起的情况下，这个公式就是要找到一个$x$，使得这个表达式最大。可以把它想象成在一堆有噪声的数据中，找到最符合整体规律的那个$x$，就像在嘈杂的环境中找到最清晰的声音。
   - Fisher 信息公式$$I=-E\frac{\partial^{2}l}{\partial x^{2}}=D_{0}^{T}\sum_{\epsilon}^{-1}D_{0}=L/\sigma_{\epsilon}^{2}$$
   这个公式是用来衡量数据中关于$x$的信息量的。$L$是前面提到的图拉普拉斯矩阵，$\sigma_{\epsilon}^{2}$是噪声的方差。它就像一个“信息探测器”，告诉我们数据中关于全局评分$x$的信息有多少。
   - 无监督采样优化目标公式$$max_{(\alpha_{t},i_{t},j_{t})}f(L_{t})$$
   这里是要找到一对要采样的选项$(i_{t},j_{t})$，使得$f(L_{t})$最大。$L_{t}$是随着采样过程不断变化的图拉普拉斯矩阵，$f$是一个根据矩阵特征设计的函数。比如说，我们可以把$f$设计成让图的连接性更好的函数，这样就能提高采样的效率。

5. **Bayesian 信息最大化（有监督采样）公式解释**
   - 正则化的 HodgeRank 问题公式
   $$min_{x}\left\|y - D_{0}x\right\|_{2}^{2}+\gamma\left\|x\right\|_{2}^{2}$$
   这个公式在原来的最小二乘法基础上增加了一个对$x$的约束（$\gamma\left\|x\right\|_{2}^{2}$），就好像在寻找全局评分时，给它加上了一个限制，让它不要太“任性”，要符合一定的规则。
   - 预期信息增益（EIG）最大化公式$$(i^{*},j^{*}) = arg max_{(i,j)}EIG_{(i,j)}$$
   这里是要找到一对选项$(i,j)$，使得从先验到后验的信息增益最大。就像在学习过程中，要找到最能让我们学到新知识的那对东西。
   - 在$\ell_{2}$正则化的 HodgeRank 设置下的相关公式：
     - 后验分布公式
     $$x|y^{t}\sim N(\mu^{t},\sigma_{\epsilon}^{2}\sum^{t})$$
     它表示在给定已经收集到的数据$y^{t}$的情况下，$x$的分布是一个正态分布，$\mu^{t}$是这个分布的均值，$\sigma_{\epsilon}^{2}\sum^{t}$是方差。这就像我们根据已经知道的信息，对未知的全局评分$x$有了一个概率上的估计。
     - KL - divergence 公式$$2KL(P^{t + 1}|P^{t})=\frac{1}{σ_{\epsilon}^{2}}(\mu^{t}-\mu^{t + 1})^{T}(L_{t}+\gamma I)(\mu^{t}-\mu^{t + 1})-n+tr((L_{t}+\gamma I)(L_{t + 1}+\gamma I)^{-1})+ln\frac{det(L_{t + 1}+\gamma I)}{det(L_{t}+\gamma I)}$$
     这个公式是用来衡量两个概率分布（先验和后验）之间的差异的。它就像一个“距离度量”，告诉我们新收集的数据对我们之前的估计有多大的改变。

6. **在线跟踪拓扑演化相关公式解释**
   - 对于持久同调监测团复形的零阶 Betti 数$\beta_{0}$（连通分量数）和一阶 Betti 数$\beta_{1}$（独立环数），可以这样理解：在我们构建的比较数据图中，如果$\beta_{0}$不为 1，说明图不是连通的，那就可能存在一些孤立的选项，无法进行有效的全局排名。而$\beta_{1}$表示独立环数，如果$\beta_{1}$不为 0，就意味着存在循环关系，可能会导致投票混乱，就像在一个环形的决策路径中，永远无法得出一个确定的结果。通过持久同调这个工具来监测这些数的变化，就像给我们的数据图安装了一个“健康监测仪”，时刻关注数据的状态，以便及时调整采样策略。

7. **实验评估相关公式解释**
   - Kendall 秩相关系数（$\tau$）：它是用来衡量我们通过各种采样方法得到的排名结果和真实排名之间的相似程度的。如果$\tau$的值接近 1，就说明我们的排名结果和真实情况很接近，采样方法比较有效；如果$\tau$的值接近 0 或为负数，就说明排名结果不太准确，需要改进采样方法。比如在电影排名中，如果我们的方法得到的排名和观众真正的喜好排名（真实排名）的$\tau$值很高，那就说明我们的方法能很好地反映观众的喜好。
   - 计算复杂度和运行成本相关的时间统计：在不同数据集规模$n$下的运行时间统计，就像我们在不同大小的任务中测试各种方法的速度。如果一种方法在大数据集（$n$很大）下运行时间很长，那就可能不太适合实际应用，因为在实际的众包任务中，我们通常需要处理大量的数据。所以通过这些时间统计，我们可以比较不同算法的效率，选择最适合的算法来进行采样和排名聚合。

